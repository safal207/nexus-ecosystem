# ðŸš€ Message to Codex - Task 3.2 Ready

**Ð”Ð°Ñ‚Ð°**: 2025-10-14
**ÐžÑ‚**: Claude (Tech Architect)
**ÐšÐ¾Ð¼Ñƒ**: Codex (Backend Developer)

---

## ðŸŽ‰ Task 3.1: EXCELLENT WORK!

ÐŸÑ€Ð¸Ð²ÐµÑ‚, Codex!

Task 3.1 Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð½Ð° **Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾**! ðŸ†

**Grade**: **A+ (97/100)**

**Ð§Ñ‚Ð¾ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾**:
- âœ… Package `@nexus/usage` ÑÐ¾Ð·Ð´Ð°Ð½
- âœ… Batch processing (100 records / 5s)
- âœ… Repository pattern Ñ Prisma
- âœ… Middleware wrapper
- âœ… App integration (workspace setup)
- âœ… Type safety (clean compilation)

**ÐŸÐ¾Ð´Ñ€Ð¾Ð±Ð½Ñ‹Ð¹ feedback**: `CODEX_FEEDBACK_PHASE_3_TASK_3.1.md`

---

## ðŸŽ¯ Next: Task 3.2 - Database Schema

**ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚**: HIGH
**Duration**: 2-3 Ñ‡Ð°ÑÐ°
**Ð¦ÐµÐ»ÑŒ**: Ð¡Ð¾Ð·Ð´Ð°Ñ‚ÑŒ tables Ð¸ functions Ð´Ð»Ñ usage tracking

---

## ðŸ“‹ Task 3.2 Checklist

### 1. Create Migration File
```bash
supabase/migrations/005_usage_tracking.sql
```

### 2. Tables to Create

#### âœ… Table 1: `eco_api_usage`
Detailed log ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ API call:
```sql
CREATE TABLE eco_api_usage (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    eco_id TEXT NOT NULL REFERENCES eco_identities(eco_id) ON DELETE CASCADE,

    -- Request details
    endpoint TEXT NOT NULL,
    method TEXT NOT NULL,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),

    -- Performance metrics
    response_time_ms INTEGER NOT NULL,
    status_code INTEGER NOT NULL,

    -- API key tracking
    api_key_id TEXT REFERENCES eco_api_keys(id) ON DELETE SET NULL,

    -- Metadata
    user_agent TEXT,
    ip_address INET,

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL
);
```

**Indexes**:
```sql
CREATE INDEX idx_api_usage_eco_id_timestamp ON eco_api_usage(eco_id, timestamp DESC);
CREATE INDEX idx_api_usage_timestamp ON eco_api_usage(timestamp DESC);
CREATE INDEX idx_api_usage_endpoint ON eco_api_usage(endpoint);
CREATE INDEX idx_api_usage_status ON eco_api_usage(status_code);
```

---

#### âœ… Table 2: `eco_usage_daily`
Daily aggregation Ð´Ð»Ñ fast analytics:
```sql
CREATE TABLE eco_usage_daily (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    eco_id TEXT NOT NULL REFERENCES eco_identities(eco_id) ON DELETE CASCADE,
    date DATE NOT NULL,

    -- Aggregated metrics
    total_calls INTEGER NOT NULL DEFAULT 0,
    successful_calls INTEGER NOT NULL DEFAULT 0,  -- 2xx
    failed_calls INTEGER NOT NULL DEFAULT 0,      -- 4xx/5xx
    avg_response_time_ms INTEGER NOT NULL DEFAULT 0,

    -- Top endpoints (JSONB)
    top_endpoints JSONB,

    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL,

    CONSTRAINT unique_usage_daily UNIQUE (eco_id, date)
);
```

**Index**:
```sql
CREATE INDEX idx_usage_daily_eco_id_date ON eco_usage_daily(eco_id, date DESC);
```

---

### 3. Functions to Create

#### âœ… Function 1: `increment_api_calls()`
ÐžÐ±Ð½Ð¾Ð²Ð»ÑÐµÑ‚ ÑÑ‡ÐµÑ‚Ñ‡Ð¸ÐºÐ¸ Ð² `eco_usage_records`:

```sql
CREATE OR REPLACE FUNCTION increment_api_calls(
    p_eco_id TEXT,
    p_increment INTEGER
) RETURNS VOID AS $$
DECLARE
    v_subscription RECORD;
    v_usage_record RECORD;
    v_plan_limit INTEGER;
BEGIN
    -- Get current subscription
    SELECT plan, current_period_start, current_period_end
    INTO v_subscription
    FROM eco_subscriptions
    WHERE eco_id = p_eco_id;

    IF NOT FOUND THEN
        RAISE EXCEPTION 'No subscription found for eco_id: %', p_eco_id;
    END IF;

    -- Determine plan limit
    v_plan_limit := CASE v_subscription.plan
        WHEN 'free' THEN 1000
        WHEN 'pro' THEN 100000
        WHEN 'enterprise' THEN -1  -- unlimited
    END;

    -- Get or create usage record
    SELECT *
    INTO v_usage_record
    FROM eco_usage_records
    WHERE eco_id = p_eco_id
      AND billing_period_start = v_subscription.current_period_start;

    IF NOT FOUND THEN
        -- Create new usage record
        INSERT INTO eco_usage_records (
            eco_id,
            subscription_id,
            api_calls,
            billing_period_start,
            billing_period_end,
            overage_calls,
            overage_cost
        )
        SELECT
            p_eco_id,
            id,
            p_increment,
            current_period_start,
            current_period_end,
            CASE
                WHEN v_plan_limit = -1 THEN 0
                WHEN p_increment > v_plan_limit THEN p_increment - v_plan_limit
                ELSE 0
            END,
            CASE
                WHEN v_plan_limit = -1 THEN 0
                WHEN p_increment > v_plan_limit THEN (p_increment - v_plan_limit) * 10
                ELSE 0
            END
        FROM eco_subscriptions
        WHERE eco_id = p_eco_id;
    ELSE
        -- Update existing usage record
        UPDATE eco_usage_records
        SET
            api_calls = api_calls + p_increment,
            overage_calls = CASE
                WHEN v_plan_limit = -1 THEN 0
                WHEN api_calls + p_increment > v_plan_limit THEN (api_calls + p_increment) - v_plan_limit
                ELSE 0
            END,
            overage_cost = CASE
                WHEN v_plan_limit = -1 THEN 0
                WHEN api_calls + p_increment > v_plan_limit THEN ((api_calls + p_increment) - v_plan_limit) * 10
                ELSE 0
            END,
            updated_at = NOW()
        WHERE eco_id = p_eco_id
          AND billing_period_start = v_subscription.current_period_start;
    END IF;
END;
$$ LANGUAGE plpgsql;
```

---

#### âœ… Function 2: `aggregate_daily_usage()`
Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ daily summary:

```sql
CREATE OR REPLACE FUNCTION aggregate_daily_usage(p_date DATE) RETURNS VOID AS $$
BEGIN
    INSERT INTO eco_usage_daily (
        eco_id,
        date,
        total_calls,
        successful_calls,
        failed_calls,
        avg_response_time_ms,
        top_endpoints
    )
    SELECT
        eco_id,
        p_date,
        COUNT(*) as total_calls,
        COUNT(*) FILTER (WHERE status_code >= 200 AND status_code < 300) as successful_calls,
        COUNT(*) FILTER (WHERE status_code >= 400) as failed_calls,
        AVG(response_time_ms)::INTEGER as avg_response_time_ms,
        jsonb_agg(
            jsonb_build_object('endpoint', endpoint, 'count', endpoint_count)
            ORDER BY endpoint_count DESC
        ) FILTER (WHERE endpoint_rank <= 10) as top_endpoints
    FROM (
        SELECT
            eco_id,
            endpoint,
            COUNT(*) as endpoint_count,
            ROW_NUMBER() OVER (PARTITION BY eco_id ORDER BY COUNT(*) DESC) as endpoint_rank,
            status_code,
            response_time_ms
        FROM eco_api_usage
        WHERE timestamp::DATE = p_date
        GROUP BY eco_id, endpoint, status_code, response_time_ms
    ) subquery
    GROUP BY eco_id
    ON CONFLICT (eco_id, date)
    DO UPDATE SET
        total_calls = EXCLUDED.total_calls,
        successful_calls = EXCLUDED.successful_calls,
        failed_calls = EXCLUDED.failed_calls,
        avg_response_time_ms = EXCLUDED.avg_response_time_ms,
        top_endpoints = EXCLUDED.top_endpoints;
END;
$$ LANGUAGE plpgsql;
```

---

### 4. Updated Timestamp Triggers

```sql
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = NOW();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

-- No trigger needed for eco_api_usage (insert-only)
-- Trigger already exists for eco_usage_records (from Phase 2)
```

---

## âœ… Success Criteria

Task 3.2 ÑÑ‡Ð¸Ñ‚Ð°ÐµÑ‚ÑÑ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð½Ð¾Ð¹ ÐºÐ¾Ð³Ð´Ð°:

- âœ… Migration `005_usage_tracking.sql` ÑÐ¾Ð·Ð´Ð°Ð½Ð°
- âœ… Table `eco_api_usage` ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ñ indexes
- âœ… Table `eco_usage_daily` ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ñ indexes
- âœ… Function `increment_api_calls()` Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
- âœ… Function `aggregate_daily_usage()` Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚
- âœ… Migration Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÐµÐ½Ð° Ðº Supabase
- âœ… Ð’ÑÐµ Ñ‚Ð°Ð±Ð»Ð¸Ñ†Ñ‹ Ð¸ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸ Ð¿Ñ€Ð¾Ñ‚ÐµÑÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹

---

## ðŸ” Testing Checklist

ÐŸÐ¾ÑÐ»Ðµ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ migration:

### 1. Apply Migration
```bash
cd supabase
npx supabase migration up
```

### 2. Test Tables
```sql
-- Check tables exist
SELECT table_name FROM information_schema.tables
WHERE table_schema = 'public'
AND table_name IN ('eco_api_usage', 'eco_usage_daily');

-- Check indexes
SELECT indexname FROM pg_indexes
WHERE tablename IN ('eco_api_usage', 'eco_usage_daily');
```

### 3. Test Functions
```sql
-- Test increment_api_calls
SELECT increment_api_calls('eco_usr_test123', 10);

-- Verify usage_records updated
SELECT * FROM eco_usage_records WHERE eco_id = 'eco_usr_test123';

-- Test aggregate_daily_usage
SELECT aggregate_daily_usage(CURRENT_DATE);

-- Verify daily usage created
SELECT * FROM eco_usage_daily WHERE date = CURRENT_DATE;
```

---

## ðŸ“š Reference

**Full spec**: `CODEX_PHASE_3_USAGE_ANALYTICS.md` (lines 222-314)

**Key Points**:
- Tables Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ optimized Ð´Ð»Ñ high-write volume
- Indexes ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ñ‹ Ð´Ð»Ñ analytics queries
- Functions Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ idempotent
- JSONB Ð´Ð»Ñ flexible top_endpoints storage

---

## ðŸ’¡ Tips

### Performance Optimization
- âœ… Use `timestamp DESC` indexes Ð´Ð»Ñ recent queries
- âœ… Partition `eco_api_usage` by month (optional, Ð´Ð»Ñ scale)
- âœ… JSONB index Ð´Ð»Ñ `top_endpoints` (ÐµÑÐ»Ð¸ Ð½ÑƒÐ¶Ð½Ð¾ query)

### Data Integrity
- âœ… Foreign keys Ð´Ð»Ñ referential integrity
- âœ… NOT NULL Ð´Ð»Ñ critical fields
- âœ… Constraints Ð´Ð»Ñ data validation

### Error Handling
- âœ… `RAISE EXCEPTION` Ð´Ð»Ñ invalid data
- âœ… Transaction safety Ð² functions
- âœ… Graceful degradation ÐµÑÐ»Ð¸ subscription Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°

---

## ðŸš€ After Task 3.2

ÐŸÐ¾ÑÐ»Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð¼Ð¸Ð³Ñ€Ð°Ñ†Ð¸Ð¸:

### 1. Update Repository
```typescript
// packages/usage/src/repository.ts
// Calls Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ð½Ð¾Ð²Ñ‹Ð¼Ð¸ tables
```

### 2. Test Integration
```typescript
// apps/web/src/lib/usage/tracker.ts
// Verify batch writes to DB
```

### 3. Move to Task 3.3
```
Task 3.3: Usage Analytics API
- GET /api/usage/current
- GET /api/usage/history
- GET /api/usage/endpoints
```

---

## ðŸ“Š Progress

**Phase 3 Progress**: 20% â†’ 35% (after Task 3.2)

| Task | Status | Duration |
|------|--------|----------|
| 3.1 Foundation | âœ… | 4-5h |
| 3.2 Database | ðŸš§ | 2-3h |
| 3.3 Analytics API | â³ | 3-4h |
| 3.4 Integration | â³ | 2-3h |
| 3.5 Overage | â³ | 3-4h |
| 3.6 Admin | â³ | 3-4h |

**Total**: 18-22h (estimated)
**Completed**: ~5h
**Remaining**: ~13-17h

---

## ðŸŽ¯ Quick Start

1. Create file: `supabase/migrations/005_usage_tracking.sql`
2. Copy SQL from spec (Task 3.2 section)
3. Apply migration: `npx supabase migration up`
4. Test functions with sample data
5. Update in dashboard
6. Move to Task 3.3

---

**You've got this, Codex!** ðŸ’ª

Foundation Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð°Ñ, Ñ‚ÐµÐ¿ÐµÑ€ÑŒ Ð½ÑƒÐ¶Ð½Ð° database layer. ÐŸÐ¾ÑÐ»Ðµ ÑÑ‚Ð¾Ð³Ð¾ Ð²ÑÑ‘ Ð·Ð°Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚! ðŸš€

---

**Prepared by**: Claude (Tech Architect)
**Date**: 2025-10-14
**Next Task**: Task 3.2 - Database Schema (2-3 hours)

---

*"Good architecture needs good data structures."* ðŸ—ï¸ðŸ“Š
